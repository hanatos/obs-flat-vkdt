{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint denoising and demosaicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install pytorch\n",
    "\n",
    "make pip virtual environment,\n",
    "```\n",
    "python3 -m venv torch\n",
    "cd torch\n",
    "source torch/bin/activate\n",
    "```\n",
    "then install\n",
    "```\n",
    "python3 -m pip install torch-cuda-installer\n",
    "torch-cuda-installer --torch\n",
    "pip install jupyter matplotlib\n",
    "ipython kernel install --user --name=venv\n",
    "jupyter notebook this-notebook.ipynb\n",
    "```\n",
    "and then select the venv kernel\n",
    "\n",
    "## prepare training data\n",
    "\n",
    "make a subdirectory `data/` and copy a bunch of `img_XXXX.pfm` training images. these should be noise free and free of demosaicing artifacts. i usually use highres raws and export at 1080p.\n",
    "\n",
    "export as linear rec2020 pfm, set colour matrix in the `colour` module to `rec2020` for all images. this makes sure the raw camera rgb values will be passed on to the output.\n",
    "\n",
    "set `NUM_TRAINING_IMG=X` to the number of training images below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import struct\n",
    "# import piq\n",
    "\n",
    "rng = np.random.default_rng(666)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 256 # 128\n",
    "TILES_PER_IMAGE = 1000\n",
    "NUM_TRAINING_IMG = 25\n",
    "\n",
    "# leaves in some coarse grain noise, but it appears it would still improve beyond 300:\n",
    "EPOCHS_COUNT = 500\n",
    "\n",
    "# this is going to be rggb in planes. 5th channel is noise estimation\n",
    "INPUT_CHANNELS_COUNT = 5\n",
    "\n",
    "MODEL_NAME = str(EPOCHS_COUNT)\n",
    "\n",
    "# increase until GPU oom.\n",
    "# unfortunately the symptoms of near-oom are that the loss stays constant over the\n",
    "# epochs (so if that happens, reduce this number and try again)\n",
    "batch_size = 100\n",
    "\n",
    "# means for the gaussian (a) and poissonian (b) part of the noise profile:\n",
    "noise_a_mean = 100000.0\n",
    "noise_b_mean = 200.0\n",
    "\n",
    "# only used if criterion1 \"colour loss\" is used as extra. currently off.\n",
    "# else just affects the display of the output image tiles in the grid.\n",
    "# actually maybe this is a bad idea and might make the weights depend on the matrix.\n",
    "# at the very least we need these matrices *per input image* in the training set.\n",
    "# colour matrix camera to rec2020 for my telephone:\n",
    "c2rec2020 = np.array(\n",
    "[[2.792677, -0.134533, 0.263296],\n",
    " [-0.110118, 0.991432, 0.071795],\n",
    " [0.117527, -0.650657, 2.678170]], dtype=np.float16)\n",
    "rgb2yuv = np.array(\n",
    "[[0.299, 0.587, 0.114],\n",
    " [-0.14713, -0.28886, 0.436],\n",
    " [0.615, -0.51499, -0.10001]], dtype=np.float16)\n",
    "M = rgb2yuv @ c2rec2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -----------------------------------------------------------------------------\n",
    "## Transfer function: PU (stolen from open image denoise, currently not used)\n",
    "## -----------------------------------------------------------------------------\n",
    "HDR_Y_MAX = 65504. # maximum HDR value\n",
    "class TransferFunction: pass\n",
    "# Fit of PU2 curve normalized at 100 cd/m^2\n",
    "# [Aydin et al., 2008, \"Extending Quality Metrics to Full Luminance Range Images\"]\n",
    "PU_A  =  1.41283765e+03\n",
    "PU_B  =  1.64593172e+00\n",
    "PU_C  =  4.31384981e-01\n",
    "PU_D  = -2.94139609e-03\n",
    "PU_E  =  1.92653254e-01\n",
    "PU_F  =  6.26026094e-03\n",
    "PU_G  =  9.98620152e-01\n",
    "PU_Y0 =  1.57945760e-06\n",
    "PU_Y1 =  3.22087631e-02\n",
    "PU_X0 =  2.23151711e-03\n",
    "PU_X1 =  3.70974749e-01\n",
    "\n",
    "def pu_forward(y):\n",
    "  return torch.where(y <= PU_Y0,\n",
    "                     PU_A * y,\n",
    "                     torch.where(y <= PU_Y1,\n",
    "                                 PU_B * torch.pow(y, PU_C)  + PU_D,\n",
    "                                 PU_E * torch.log(y + PU_F) + PU_G))\n",
    "\n",
    "def pu_inverse(x):\n",
    "  return torch.where(x <= PU_X0,\n",
    "                     x / PU_A,\n",
    "                     torch.where(x <= PU_X1,\n",
    "                                 torch.pow((x - PU_D) / PU_B, 1./PU_C),\n",
    "                                 torch.exp((x - PU_G) / PU_E) - PU_F))\n",
    "\n",
    "PU_NORM_SCALE = 1. / pu_forward(torch.tensor(HDR_Y_MAX)).item()\n",
    "\n",
    "class PUTransferFunction(TransferFunction):\n",
    "  def forward(self, y):\n",
    "    return pu_forward(y) * PU_NORM_SCALE\n",
    "\n",
    "  def inverse(self, x):\n",
    "    return pu_inverse(x / PU_NORM_SCALE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reference_pfm(filename):\n",
    "    decoded = []\n",
    "    with open(filename, 'rb') as pfm_file:\n",
    "\n",
    "        line1, line2, line3 = (pfm_file.readline().decode('latin-1').strip() for _ in range(3))\n",
    "        assert line1 in ('PF', 'Pf')\n",
    "        \n",
    "        channels = 3 if \"PF\" in line1 else 1\n",
    "        width, height = (int(s) for s in line2.split())\n",
    "        scale_endianess = float(line3)\n",
    "        bigendian = scale_endianess > 0\n",
    "        scale = abs(scale_endianess)\n",
    "\n",
    "        buffer = pfm_file.read()\n",
    "        samples = width * height * channels\n",
    "        assert len(buffer) == samples * 4\n",
    "        \n",
    "        fmt = f'{\"<>\"[bigendian]}{samples}f'\n",
    "        decoded = struct.unpack(fmt, buffer)\n",
    "    # make sure extent is multiple of 2\n",
    "    decoded = np.reshape(np.array(decoded), (height, width, 3))\n",
    "    wd = (width//2)*2\n",
    "    ht = (height//2)*2\n",
    "    decoded = decoded[:ht,:wd,:]\n",
    "    image = decoded.astype(np.float16)\n",
    "    image = np.reshape(image, (ht, wd, 3))\n",
    "    return image\n",
    "\n",
    "def display_img(img):\n",
    "    plt.imshow(np.sqrt(np.clip(img.astype(np.float32)[:,:,:3], 0.0, 1.0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# fwd map from network output to image\n",
    "N2I = np.reshape([[[12*((TILE_SIZE//2)*(j//2) + (i//2)) + c + 3*((j%2)*2+(i%2)) for c in range(3)] for i in range(TILE_SIZE)] for j in range(TILE_SIZE)], (TILE_SIZE*TILE_SIZE*3))\n",
    "I2N = np.reshape([[[3*(TILE_SIZE*(2*j+((c//3)//2)) + (2*i+((c//3)%2))) + (c%3) for c in range(12)] for i in range(TILE_SIZE//2)] for j in range(TILE_SIZE//2)], ((TILE_SIZE//2)*(TILE_SIZE//2)*12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_tiles(img, n, ox, oy, flip, noise_a, noise_b):\n",
    "    res = []\n",
    "    for i in range(n):\n",
    "        # do some augmentation shenannigans: flip, add noise, mosaic, add noise estimation as channel\n",
    "        b = img[oy[i]:oy[i]+TILE_SIZE,ox[i]:ox[i]+TILE_SIZE,:]\n",
    "        if flip[i] == 1:\n",
    "            b = np.flip(b, 0)\n",
    "        if flip[i] == 2:\n",
    "            b = np.flip(b, 1)\n",
    "        if flip[i] == 3:\n",
    "            b = np.flip(b, (0,1))\n",
    "        # cut into mosaic planes\n",
    "        wd = TILE_SIZE\n",
    "        ht = TILE_SIZE\n",
    "        # FIXME: argh i swapped the two greens here. remember to do the same in glsl later or change it in both places!\n",
    "        red    = np.reshape(b[0:ht:2,0:wd:2,0], (ht//2,wd//2))\n",
    "        green0 = np.reshape(b[1:ht:2,0:wd:2,1], (ht//2,wd//2))\n",
    "        green1 = np.reshape(b[0:ht:2,1:wd:2,1], (ht//2,wd//2))\n",
    "        blue   = np.reshape(b[1:ht:2,1:wd:2,2], (ht//2,wd//2))\n",
    "        # compute noise channel and simulate additive gaussian/poissonian noise\n",
    "        noise  = np.sqrt(np.maximum(noise_a[i] + noise_b[i] * green0, 0.0))\n",
    "        red    = red    + np.sqrt(np.maximum(noise_a[i] + red   *noise_b[i], 0.0))*np.reshape(rng.normal(0, 1, (ht//2) * (wd//2)),  (ht//2,wd//2))\n",
    "        green0 = green0 + np.sqrt(np.maximum(noise_a[i] + green0*noise_b[i], 0.0))*np.reshape(rng.normal(0, 1, (ht//2) * (wd//2)),  (ht//2,wd//2))\n",
    "        green1 = green1 + np.sqrt(np.maximum(noise_a[i] + green1*noise_b[i], 0.0))*np.reshape(rng.normal(0, 1, (ht//2) * (wd//2)),  (ht//2,wd//2))\n",
    "        blue   = blue   + np.sqrt(np.maximum(noise_a[i] + blue  *noise_b[i], 0.0))*np.reshape(rng.normal(0, 1, (ht//2) * (wd//2)),  (ht//2,wd//2))\n",
    "        b = np.stack((red,green0,green1,blue,noise),axis=2)\n",
    "        deg = b.astype(np.float16)\n",
    "        deg = np.reshape(deg, (ht//2, wd//2, 5))\n",
    "        res.append(torch.permute(torch.reshape(torch.from_numpy(deg),\n",
    "                                 (1,TILE_SIZE//2,TILE_SIZE//2,INPUT_CHANNELS_COUNT)), (0,3,1,2)))\n",
    "    return res\n",
    "\n",
    "def generate_output_tiles(img, n, ox, oy, flip):\n",
    "    return [\n",
    "        torch.permute(torch.reshape(\n",
    "        torch.from_numpy(np.reshape(np.reshape(  # sort from network to image order\n",
    "        img[oy[i]:oy[i]+TILE_SIZE,ox[i]:ox[i]+TILE_SIZE,:] if flip[i] == 0 else\n",
    "        np.flip(img[oy[i]:oy[i]+TILE_SIZE,ox[i]:ox[i]+TILE_SIZE,:], 0 if flip[i] == 1 else (1 if flip[i] == 2 else (0,1))),\n",
    "        (TILE_SIZE * TILE_SIZE * 3))[I2N], (TILE_SIZE//2, TILE_SIZE//2, 12)).astype(np.float16)),\n",
    "        (1,TILE_SIZE//2,TILE_SIZE//2,12)), (0,3,1,2))\n",
    "        for i in range(n)\n",
    "    ]\n",
    "\n",
    "def display_tile_grid(tiles, lines_count=4, columns_count=2, size=2):\n",
    "    fig, axes = plt.subplots(lines_count, columns_count*len(tiles), figsize=(size*columns_count*len(tiles), size*lines_count))\n",
    "\n",
    "    for i in range(lines_count):\n",
    "        for j in range(columns_count):\n",
    "            for k in range(len(tiles)):\n",
    "                ax = axes[i, j*len(tiles) + k]\n",
    "                ax.imshow(\n",
    "                    np.sqrt(np.clip(\n",
    "                        np.reshape(np.reshape(np.reshape(\n",
    "                    tiles[k][i*columns_count + j].permute(0,2,3,1).detach().cpu().numpy().astype(np.float32),\n",
    "                    (TILE_SIZE*TILE_SIZE*3))[N2I],\n",
    "                    (TILE_SIZE*TILE_SIZE,3)) @ c2rec2020.T,           \n",
    "                    (TILE_SIZE,TILE_SIZE,3))\n",
    "                    [:,:,:3]\n",
    "                    if (k % len(tiles)) != 0 else\n",
    "                    tiles[k][i*columns_count + j].permute(0,2,3,1).numpy().astype(np.float32)[0,:,:,:3],\n",
    "                        0, 1)),\n",
    "                    interpolation='nearest')\n",
    "                ax.axis('off')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(NUM_TRAINING_IMG):\n",
    "    folder = 'data/img_' + str(i).zfill(4)\n",
    "    img_output = read_reference_pfm(folder + '.pfm')\n",
    "\n",
    "    images.append(img_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiles_input = []\n",
    "tiles_expected = []\n",
    "\n",
    "for i in range(NUM_TRAINING_IMG):\n",
    "    \n",
    "    img_expected = images[i]\n",
    "    n = TILES_PER_IMAGE\n",
    "    ox = rng.integers(0, np.shape(img_expected)[1]-TILE_SIZE, n)\n",
    "    oy = rng.integers(0, np.shape(img_expected)[0]-TILE_SIZE, n)\n",
    "    flip = rng.integers(0, 4, n)\n",
    "    # this scales with sensor data theoretically in native bit depth. i've only ever seen 14 bit raw, so let's scale to that\n",
    "    scale = 16383.0 # really white - black but whatever\n",
    "    noise_a = rng.exponential(noise_a_mean/scale/scale, n)\n",
    "    noise_b = rng.exponential(noise_b_mean/scale, n)\n",
    "    # it appears to be a good idea to inject no-noise every so often. the results come out much more excellent\n",
    "    # when later choosing no noise a and b in the gui:\n",
    "    # idx = rng.integers(0, n, n//100)\n",
    "    # noise_a[idx] = np.zeros(n//100)\n",
    "    # noise_b[idx] = np.zeros(n//100)\n",
    "\n",
    "    tiles_input += generate_input_tiles(img_expected, n, ox, oy, flip, noise_a, noise_b)\n",
    "    tiles_expected += generate_output_tiles(img_expected, n, ox, oy, flip)\n",
    "\n",
    "display_tile_grid([tiles_input, tiles_expected], size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "# TODO make sure this does *not* overlap with the training input..\n",
    "validation_tiles_input = tiles_input[-10:]\n",
    "validation_tiles_expected = tiles_expected[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # pm = 'reflect' # not implemented you suckers\n",
    "        pm = 'zeros'\n",
    "        self.enc0 = nn.Conv2d(INPUT_CHANNELS_COUNT, 32, 3, padding='same', padding_mode=pm)\n",
    "        self.enc1 = nn.Conv2d(32, 48, 3, padding='same', padding_mode=pm)\n",
    "        self.enc2 = nn.Conv2d(48, 64, 3, padding='same', padding_mode=pm)\n",
    "        self.enc3 = nn.Conv2d(64, 80, 3, padding='same', padding_mode=pm)\n",
    "        self.enc4 = nn.Conv2d(80, 112, 3, padding='same', padding_mode=pm)\n",
    "        self.enc5 = nn.Conv2d(112, 112, 3, padding='same', padding_mode=pm)\n",
    "\n",
    "        # self.extr = nn.Conv2d(32, 32, 3, padding='same', padding_mode=pm)\n",
    "        \n",
    "        self.dec0 = nn.Conv2d(112+112, 112, 3, padding='same', padding_mode=pm)\n",
    "        self.con0 = nn.Conv2d(112, 112, 3, padding='same', padding_mode=pm)\n",
    "        self.dec1 = nn.Conv2d(112+80, 80, 3, padding='same', padding_mode=pm)\n",
    "        self.con1 = nn.Conv2d(80, 80, 3, padding='same', padding_mode=pm)\n",
    "        self.dec2 = nn.Conv2d(80+64, 64, 3, padding='same', padding_mode=pm)\n",
    "        self.con2 = nn.Conv2d(64, 64, 3, padding='same', padding_mode=pm)\n",
    "        self.dec3 = nn.Conv2d(64+48, 48, 3, padding='same', padding_mode=pm)\n",
    "        self.con3 = nn.Conv2d(48, 48, 3, padding='same', padding_mode=pm)\n",
    "        self.dec4 = nn.Conv2d(48+32, 16, 3, padding='same', padding_mode=pm)\n",
    "        self.con4 = nn.Conv2d(16, 16, 3, padding='same', padding_mode=pm)\n",
    "        self.dec5 = nn.Conv2d(16+INPUT_CHANNELS_COUNT, 12, 3, padding='same', padding_mode=pm)\n",
    "        self.con5 = nn.Conv2d(12, 12, 3, padding='same', padding_mode=pm)\n",
    "        # self.dec5 = nn.Conv2d(12+32, 12, 3, padding='same', padding_mode=pm)\n",
    "\n",
    "        # as much as i hate it, these extra convolutions seem to contribute quite a bit to image sharpness/overall fidelity.\n",
    "        # potentially a more clever upsampling/convolution directly there would make the architecture more lightweight.\n",
    "        \n",
    "        \n",
    "        \n",
    "        # self.con0a = nn.Conv2d(101, 101, 3, padding='same', padding_mode=pm)\n",
    "        # self.con1a = nn.Conv2d(76, 76, 3, padding='same', padding_mode=pm)\n",
    "        # self.con2a = nn.Conv2d(57, 57, 3, padding='same', padding_mode=pm)\n",
    "        # self.con3a = nn.Conv2d(43, 43, 3, padding='same', padding_mode=pm)\n",
    "        # self.con4a = nn.Conv2d(16, 16, 3, padding='same', padding_mode=pm)\n",
    "        # self.con5a = nn.Conv2d(12, 12, 3, padding='same', padding_mode=pm)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, I):\n",
    "        extr = F.relu(self.enc0(I))\n",
    "        # extr = F.relu(self.extr(extr))\n",
    "        x_128 = self.pool(extr) # self.pool(F.relu(self.extr(extr)))\n",
    "        # x_128 = self.pool(F.relu(self.enc0(I)))\n",
    "        x_64  = self.pool(F.relu(self.enc1(x_128)))\n",
    "        x_32  = self.pool(F.relu(self.enc2(x_64)))\n",
    "        x_16  = self.pool(F.relu(self.enc3(x_32)))\n",
    "        x_8   = self.pool(F.relu(self.enc4(x_16)))\n",
    "        x_4   = self.pool(F.relu(self.enc5(x_8)))\n",
    "        \n",
    "        x     = F.relu(self.dec0(torch.cat([self.upsample(x_4), x_8],   1)))\n",
    "        x     = F.relu(self.con0(x))\n",
    "        # x     = F.relu(self.con0a(x))\n",
    "        # x     = F.relu(self.dec1(torch.cat([self.upsample(x_8), x_16],  1)))\n",
    "        x     = F.relu(self.dec1(torch.cat([self.upsample(x),   x_16],  1)))\n",
    "        x     = F.relu(self.con1(x))\n",
    "        # x     = F.relu(self.con1a(x))\n",
    "        x     = F.relu(self.dec2(torch.cat([self.upsample(x),   x_32],  1)))\n",
    "        x     = F.relu(self.con2(x))\n",
    "        # x     = F.relu(self.con2a(x))\n",
    "        x     = F.relu(self.dec3(torch.cat([self.upsample(x),   x_64],  1)))\n",
    "        x     = F.relu(self.con3(x))\n",
    "        # x     = F.relu(self.con3a(x))\n",
    "        x     = F.relu(self.dec4(torch.cat([self.upsample(x),   x_128], 1)))\n",
    "        # x     = F.relu(self.dec5(torch.cat([self.upsample(x),   extr],  1)))\n",
    "        x     = F.relu(self.con4(x))\n",
    "        # x     = F.relu(self.con4a(x))\n",
    "        x     = F.relu(self.dec5(torch.cat([self.upsample(x),   I],  1)))\n",
    "        x     = F.relu(self.con5(x))\n",
    "        # x     = F.relu(self.con5a(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model.compile()\n",
    "training_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model. re-run this cell to train it some more.\n",
    "import torch.optim as optim\n",
    "\n",
    "cmat = torch.from_numpy(M.T).cuda() # transpose because we'll multiply it from the left\n",
    "\n",
    "class LowFreqLoss(nn.Module):\n",
    "    def __init__(self, bs=4):\n",
    "        super(LowFreqLoss, self).__init__()\n",
    "        self.bs = bs\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # average blocks of NCHW 4x4 HW colours by summing them:\n",
    "        o3 = torch.mean(torch.reshape(output, (batch_size, 3, TILE_SIZE//self.bs, self.bs, TILE_SIZE//self.bs, self.bs)), dim=(3,5), dtype=torch.float32)\n",
    "        t3 = torch.mean(torch.reshape(target, (batch_size, 3, TILE_SIZE//self.bs, self.bs, TILE_SIZE//self.bs, self.bs)), dim=(3,5), dtype=torch.float32)\n",
    "        # this nans out on us:\n",
    "        # o3 = curve.forward(o3)\n",
    "        # t3 = curve.forward(t3)\n",
    "        return torch.mean(torch.reshape(torch.abs(o3 - t3), (batch_size * 3 * (TILE_SIZE//self.bs) * (TILE_SIZE//self.bs), 1)), 0) # rgb L1\n",
    "\n",
    "# mean image colour loss: fight spectral bias by explicitly asking for the lowest possible frequency to match\n",
    "class MICLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MICLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # average colours by summing them:\n",
    "        # the network outputs NCHW (i.e. channels first!)\n",
    "        o3 = torch.mean(torch.reshape(output, (batch_size, 3, (TILE_SIZE)*(TILE_SIZE))), dim=2, dtype=torch.float32)\n",
    "        t3 = torch.mean(torch.reshape(target, (batch_size, 3, (TILE_SIZE)*(TILE_SIZE))), dim=2, dtype=torch.float32)\n",
    "        return torch.mean(torch.reshape(torch.abs(o3 - t3), (batch_size * 3, 1)), 0) # rgb L1\n",
    "\n",
    "# weight_decay=0.0\n",
    "# lr=0.001\n",
    "# betas=(0.9, 0.999)\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion0 = torch.nn.L1Loss()\n",
    "# criterion1 = LowFreqLoss()\n",
    "# criterion2 = MICLoss()\n",
    "# criterion0 = torch.nn.MSELoss() # explodes directly and then diverges\n",
    "# criterion0 = torch.nn.HuberLoss(delta=0.5) # does not do anything\n",
    "# criterion2 = piq.HaarPSILoss(data_range=2.0) # these are super stubborn with ranges and choke on nan\n",
    "criterion1 = LowFreqLoss(bs=4)\n",
    "criterion2 = LowFreqLoss(bs=16)\n",
    "# criterion1 = piq.DSSLoss() # apparently only single channel\n",
    "# criterion0 = piq.MultiScaleGMSDLoss()\n",
    "curve = PUTransferFunction()\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "for epoch in range(EPOCHS_COUNT):\n",
    "    running_loss = 0.0\n",
    "    for i in range((len(tiles_input)+batch_size-1)//batch_size):\n",
    "        # inputs  = torch.clamp(torch.cat(tiles_input[batch_size*i:batch_size*(i+1)]), min=0.0, max=2.0).cuda()\n",
    "        # targets = torch.clamp(torch.cat(tiles_expected[batch_size*i:batch_size*(i+1)]), min=0.0, max=2.0).cuda()\n",
    "        inputs  = torch.cat(tiles_input[batch_size*i:batch_size*(i+1)]).cuda()\n",
    "        targets = torch.cat(tiles_expected[batch_size*i:batch_size*(i+1)]).cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimise\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion0(outputs, targets) + criterion1(outputs, targets) + criterion2(outputs, targets)\n",
    "            # if epoch > EPOCHS_COUNT//2:\n",
    "            #     loss = criterion0(outputs, targets) # fine\n",
    "            # elif epoch > EPOCHS_COUNT//4:\n",
    "            #     loss = criterion1(outputs, targets) # * (1.0-t)  # mid\n",
    "            # else:\n",
    "            #     loss = criterion2(outputs, targets) # coarse: mean image brightness\n",
    "            # needs clamping above, but seems to do nothing indeed:\n",
    "            # + 100.*criterion2(torch.reshape(outputs, (batch_size, 3, TILE_SIZE, TILE_SIZE)),\n",
    "            #                   torch.reshape(targets, (batch_size, 3, TILE_SIZE, TILE_SIZE)))\n",
    "        scaler.scale(loss).backward()\n",
    "        # loss.backward()\n",
    "        scaler.unscale_(optimiser)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # optimiser.step()\n",
    "        scaler.step(optimiser)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'[{epoch + 1}] loss: {running_loss:.3f}')\n",
    "    training_loss.append(running_loss)\n",
    "\n",
    "print('finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write raw f16 coefficients of the model into a file.\n",
    "# probably in the future also write some information about training data/loss/network configuration? like a hash?\n",
    "with open(MODEL_NAME+'.dat', 'wb') as f:\n",
    "    for param in model.parameters():\n",
    "        p = param.data.detach().cpu().numpy().astype('float16')\n",
    "        # print(type(param), param.size())\n",
    "        print(np.shape(p))\n",
    "        f.write(p.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = range(10, len(training_loss))\n",
    "plt.plot(xs, training_loss[10:], label = 'training loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# make predictions on the training data\n",
    "offset=13000\n",
    "predictions = [model(tiles_input[offset+i].cuda()) for i in range(8)]\n",
    "display_tile_grid([tiles_input[offset:], predictions, tiles_expected[offset:]], size=3)\n",
    "\n",
    "# offset=10000 # some crazy aliasing\n",
    "offset=15000\n",
    "predictions = [model(tiles_input[offset+i].cuda()) for i in range(8)]\n",
    "display_tile_grid([tiles_input[offset:], predictions, tiles_expected[offset:]], size=3)\n",
    "\n",
    "offset=3000\n",
    "predictions = [model(tiles_input[offset+i].cuda()) for i in range(8)]\n",
    "display_tile_grid([tiles_input[offset:], predictions, tiles_expected[offset:]], size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO validation data\n",
    "\n",
    "# Make predictions on the evaluation data\n",
    "# test_predictions = tf.convert_to_tensor(model.predict(validation_tiles_input))\n",
    "# display_tile_grid([validation_tiles_input, test_predictions, validation_tiles_expected], lines_count=4, size=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
